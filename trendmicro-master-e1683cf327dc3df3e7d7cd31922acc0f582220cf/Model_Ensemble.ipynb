{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''Basic package'''\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "import queue\n",
    "import cv2          #影像處理\n",
    "import scipy.misc   #影像處理\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm_notebook as tqdm #進度條\n",
    "import matplotlib.pyplot as plt #繪圖\n",
    "import datetime\n",
    "import xgboost\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#抓入之前產出的2張表，還有training-set.csv和testing-set.csv\n",
    "file_stat = pd.read_csv('total_data_analysis.csv')\n",
    "#把index欄位drop掉\n",
    "label =pd.read_csv('/data/examples/trend/data/training-set.csv',\n",
    "                   names =('FileID','label'),\n",
    "                   header=None)\n",
    "pivot_table_prod = pd.read_csv('pivot_table_prod.csv')\n",
    "pivot_table_date = pd.read_csv('pivot_table_date.csv')\n",
    "pivot_table_hour = pd.read_csv('pivot_table_hour.csv')\n",
    "pivot_table_week = pd.read_csv('pivot_table_week.csv')\n",
    "\n",
    "submission = pd.read_csv('/data/examples/trend/data/testing-set.csv',\n",
    "                   names =('FileID','label'),\n",
    "                   header=None)\n",
    "#分別把testing跟training的label跟兩張表merge在一起\n",
    "pivot_merge = pd.merge(pd.merge(pd.merge(pivot_table_prod,pivot_table_date,how='inner',on='FileID'),\n",
    "                                pivot_table_hour,how='inner',on='FileID'),pivot_table_week,how='inner',on='FileID')\n",
    "testing_data= pd.merge(submission,pd.merge(file_stat,pivot_merge,how='inner',on='FileID'),how='inner',on='FileID')\n",
    "merge = pd.merge(pd.merge(file_stat,pivot_merge,how='inner',on='FileID'),label,how='inner',on='FileID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5640, 164) (46878, 164)\n",
      "(846, 164) (7032, 164)\n",
      "(4794, 164) (39846, 164)\n"
     ]
    }
   ],
   "source": [
    "#因為病毒資料數較少，為了避免直接做train_test_split會分布不均，所以各自病毒跟正常的FileID分開\n",
    "malware = merge.loc[merge['label']==1]\n",
    "normal = merge.loc[merge['label']==0]\n",
    "print(malware.shape,normal.shape)\n",
    "#針對DF作取樣20%\n",
    "malware_test = malware.sample(frac=0.15)\n",
    "#把取樣取出的樣本從訓練及拿掉\n",
    "malware_train = malware.drop(malware_test.iloc[:,0:0].index.tolist())\n",
    "#針對DF作取樣20%\n",
    "normal_test = normal.sample(frac=0.15)\n",
    "#把取樣取出的樣本從訓練及拿掉\n",
    "normal_train = normal.drop(normal_test.iloc[:,0:0].index.tolist())\n",
    "\n",
    "print(malware_test.shape,normal_test.shape)\n",
    "print(malware_train.shape,normal_train.shape)\n",
    "#把病毒跟正常檔案合併回來\n",
    "df_train = normal_train.append(malware_train).sample(frac=1)\n",
    "df_test = normal_test.append(malware_test).sample(frac=1)\n",
    "#把最後一個column：label去除、x_train跟x_test還要把FileID拿掉\n",
    "x_train = df_train.iloc[:,1:-1]\n",
    "\n",
    "#feature_name抓出來，理論上在XGBoost的時候可以把column name抓回去畫圖，但還不會做\n",
    "feature_name = x_train.columns\n",
    "\n",
    "x_test = df_test.iloc[:,1:-1]\n",
    "y_train = df_train.iloc[:,-1]\n",
    "y_test = df_test.iloc[:,-1]\n",
    "\n",
    "#轉成array（因為training好像是吃array）\n",
    "x_train = x_train.as_matrix()\n",
    "x_test = x_test.as_matrix()\n",
    "y_train = y_train.as_matrix()\n",
    "y_test = y_test.as_matrix()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rf = RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
    "            max_depth=10, max_features=None, max_leaf_nodes=None,\n",
    "             min_samples_leaf=1,\n",
    "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
    "            n_estimators=50, n_jobs=1, oob_score=False, random_state=None,\n",
    "            verbose=0, warm_start=False)\n",
    "# model_rf.fit(x_train, y_train)\n",
    "# y_pred_rf = model_rf.predict_proba(x_test)\n",
    "# y_pred_rf=y_pred_rf[:,1]\n",
    "# fpr, tpr, thresholds = sklearn.metrics.roc_curve(y_test, y_pred_rf)\n",
    "# sklearn.metrics.auc(fpr,tpr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GradientBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_gb = GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
    "              learning_rate=0.05, loss='deviance', max_depth=6,\n",
    "              max_features=None, max_leaf_nodes=None,\n",
    "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "              min_samples_leaf=1, min_samples_split=2,\n",
    "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
    "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
    "              warm_start=False)\n",
    "# model_gb.fit(x_train, y_train)\n",
    "# y_pred_gb = model_gb.predict_proba(x_test)\n",
    "# y_pred_gb=y_pred_gb[:,1]\n",
    "# fpr, tpr, thresholds = sklearn.metrics.roc_curve(y_test, y_pred_gb)\n",
    "# sklearn.metrics.auc(fpr,tpr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_set =  [(x_test, y_test)]\n",
    "model_xgb = xgboost.XGBClassifier(learning_rate=0.2,max_depth=8,n_estimators=200,\n",
    "       reg_alpha=0, reg_lambda=1)\n",
    "# model_xgb.fit(x_train, y_train,early_stopping_rounds=10, eval_metric=\"auc\",\n",
    "#           eval_set=eval_set, verbose=True)\n",
    "# y_pred_xgb = model_xgb.predict_proba(x_test)\n",
    "# y_pred_xgb=y_pred_xgb[:,1]\n",
    "# fpr, tpr, thresholds = sklearn.metrics.roc_curve(y_test, y_pred_xgb)\n",
    "# sklearn.metrics.auc(fpr,tpr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "eclf = VotingClassifier(estimators=[('gb', model_gb), ('rf', model_rf), ('xbg', model_xgb)], voting='soft', weights=[1,1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eclf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_ensem=eclf.predict_proba(x_test)\n",
    "y_pred_ensem=y_pred_ensem[:,1]\n",
    "fpr, tpr, thresholds = sklearn.metrics.roc_curve(y_test, y_pred_ensem)\n",
    "sklearn.metrics.auc(fpr,tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#因為submission原本抓進來是按照FileID排序，但在前面merge的時候亂掉，這裡要先用sort把他換回來\n",
    "testing_data=testing_data.sort_values('FileID')\n",
    "#去除FileID跟label的欄位\n",
    "testing_set=testing_data.drop(['FileID','label'],axis=1)\n",
    "#轉成array（因為training好像是吃array）\n",
    "testing_set=testing_set.as_matrix()\n",
    "#predict_proba出來才會是百分比\n",
    "predict = eclf.predict_proba(testing_set)\n",
    "predict = predict[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict\n",
    "submission['Probability']=predict\n",
    "submission=submission.drop(['label'],axis=1)\n",
    "submission.columns=['FileID','Probability']\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#匯出\n",
    "submission.to_csv('submission.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
