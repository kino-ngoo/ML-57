{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "'''Basic package'''\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "import queue\n",
    "import cv2          #影像處理\n",
    "import scipy.misc   #影像處理\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm_notebook as tqdm #進度條\n",
    "import matplotlib.pyplot as plt #繪圖\n",
    "import datetime\n",
    "import xgboost\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "% pylab inline\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#抓入之前產出的2張表，還有training-set.csv和testing-set.csv\n",
    "file_stat = pd.read_csv('total_data_analysis.csv')\n",
    "#把index欄位drop掉\n",
    "label =pd.read_csv('/data/examples/trend/data/training-set.csv',\n",
    "                   names =('FileID','label'),\n",
    "                   header=None)\n",
    "pivot_table_prod = pd.read_csv('pivot_table_prod.csv')\n",
    "pivot_table_date = pd.read_csv('pivot_table_date.csv')\n",
    "pivot_table_hour = pd.read_csv('pivot_table_hour.csv')\n",
    "pivot_table_week = pd.read_csv('pivot_table_week.csv')\n",
    "\n",
    "submission = pd.read_csv('/data/examples/trend/data/testing-set.csv',\n",
    "                   names =('FileID','label'),\n",
    "                   header=None)\n",
    "#分別把testing跟training的label跟兩張表merge在一起\n",
    "pivot_merge = pd.merge(pd.merge(pd.merge(pivot_table_prod,pivot_table_date,how='inner',on='FileID'),\n",
    "                                pivot_table_hour,how='inner',on='FileID'),pivot_table_week,how='inner',on='FileID')\n",
    "testing_data= pd.merge(submission,pd.merge(file_stat,pivot_merge,how='inner',on='FileID'),how='inner',on='FileID')\n",
    "merge = pd.merge(pd.merge(file_stat,pivot_merge,how='inner',on='FileID'),label,how='inner',on='FileID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5640, 164) (46878, 164)\n",
      "(1128, 164) (9376, 164)\n",
      "(4512, 164) (37502, 164)\n"
     ]
    }
   ],
   "source": [
    "#因為病毒資料數較少，為了避免直接做train_test_split會分布不均，所以各自病毒跟正常的FileID分開\n",
    "malware = merge.loc[merge['label']==1]\n",
    "normal = merge.loc[merge['label']==0]\n",
    "print(malware.shape,normal.shape)\n",
    "#針對DF作取樣20%\n",
    "malware_test = malware.sample(frac=0.2)\n",
    "#把取樣取出的樣本從訓練及拿掉\n",
    "malware_train = malware.drop(malware_test.iloc[:,0:0].index.tolist())\n",
    "#針對DF作取樣20%\n",
    "normal_test = normal.sample(frac=0.2)\n",
    "#把取樣取出的樣本從訓練及拿掉\n",
    "normal_train = normal.drop(normal_test.iloc[:,0:0].index.tolist())\n",
    "\n",
    "print(malware_test.shape,normal_test.shape)\n",
    "print(malware_train.shape,normal_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#把病毒跟正常檔案合併回來\n",
    "df_train = normal_train.append(malware_train).sample(frac=1)\n",
    "df_test = normal_test.append(malware_test).sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((42014, 164), (10504, 164))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape,df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#把最後一個column：label去除、x_train跟x_test還要把FileID拿掉\n",
    "x_train = df_train.iloc[:,1:-1]\n",
    "\n",
    "#feature_name抓出來，理論上在XGBoost的時候可以把column name抓回去畫圖，但還不會做\n",
    "feature_name = x_train.columns\n",
    "\n",
    "x_test = df_test.iloc[:,1:-1]\n",
    "y_train = df_train.iloc[:,-1]\n",
    "y_test = df_test.iloc[:,-1]\n",
    "\n",
    "#轉成array（因為training好像是吃array）\n",
    "x_train = x_train.as_matrix()\n",
    "x_test = x_test.as_matrix()\n",
    "y_train = y_train.as_matrix()\n",
    "y_test = y_test.as_matrix()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((42014, 162), (42014,), (10504, 162), (10504,))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shape(x_train),shape(y_train),shape(x_test),shape(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 128)               20864     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 513       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 185,985\n",
      "Trainable params: 185,985\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "num_classes = 2\n",
    "epochs = 30\n",
    "model_name = 'trendmicro'\n",
    "\n",
    "# Convert class vectors to binary class matrices.\n",
    "# y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "# y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "model_dnn = Sequential()\n",
    "model_dnn.add(Dense(128, input_dim=x_train.shape[1], activation='relu'))\n",
    "model_dnn.add(Dense(256))\n",
    "model_dnn.add(Activation('sigmoid'))\n",
    "model_dnn.add(Dropout(0.3))\n",
    "model_dnn.add(Dense(512))\n",
    "model_dnn.add(Activation('sigmoid'))\n",
    "model_dnn.add(Dropout(0.3))\n",
    "model_dnn.add(Dense(1))\n",
    "model_dnn.add(Activation('sigmoid'))\n",
    "\n",
    "print(model_dnn.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# initiate Adagrad optimizer\n",
    "opt = keras.optimizers.Adagrad()\n",
    "\n",
    "# Let's train the model using Adam\n",
    "model_dnn.compile(loss='binary_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['binary_accuracy'])\n",
    "\n",
    "# earlystop\n",
    "earlystop = EarlyStopping(monitor='val_loss', patience=5, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 42014 samples, validate on 10504 samples\n",
      "Epoch 1/30\n",
      "42014/42014 [==============================] - 22s 535us/step - loss: 0.2789 - binary_accuracy: 0.8944 - val_loss: 0.2548 - val_binary_accuracy: 0.9015\n",
      "Epoch 2/30\n",
      "42014/42014 [==============================] - 18s 420us/step - loss: 0.2470 - binary_accuracy: 0.9059 - val_loss: 0.2377 - val_binary_accuracy: 0.9079\n",
      "Epoch 3/30\n",
      "42014/42014 [==============================] - 17s 407us/step - loss: 0.2307 - binary_accuracy: 0.9128 - val_loss: 0.2273 - val_binary_accuracy: 0.9132\n",
      "Epoch 4/30\n",
      "42014/42014 [==============================] - 13s 307us/step - loss: 0.2209 - binary_accuracy: 0.9180 - val_loss: 0.2216 - val_binary_accuracy: 0.9160\n",
      "Epoch 5/30\n",
      "42014/42014 [==============================] - 19s 442us/step - loss: 0.2117 - binary_accuracy: 0.9220 - val_loss: 0.2178 - val_binary_accuracy: 0.9182\n",
      "Epoch 6/30\n",
      "42014/42014 [==============================] - 17s 410us/step - loss: 0.2041 - binary_accuracy: 0.9250 - val_loss: 0.2152 - val_binary_accuracy: 0.9201\n",
      "Epoch 7/30\n",
      "42014/42014 [==============================] - 17s 398us/step - loss: 0.1981 - binary_accuracy: 0.9284 - val_loss: 0.2150 - val_binary_accuracy: 0.9202\n",
      "Epoch 8/30\n",
      "42014/42014 [==============================] - 17s 416us/step - loss: 0.1927 - binary_accuracy: 0.9320 - val_loss: 0.2095 - val_binary_accuracy: 0.9214\n",
      "Epoch 9/30\n",
      "42014/42014 [==============================] - 17s 395us/step - loss: 0.1885 - binary_accuracy: 0.9315 - val_loss: 0.2078 - val_binary_accuracy: 0.9221\n",
      "Epoch 10/30\n",
      "42014/42014 [==============================] - 18s 424us/step - loss: 0.1854 - binary_accuracy: 0.9332 - val_loss: 0.2079 - val_binary_accuracy: 0.9236\n",
      "Epoch 11/30\n",
      "42014/42014 [==============================] - 18s 428us/step - loss: 0.1811 - binary_accuracy: 0.9350 - val_loss: 0.2080 - val_binary_accuracy: 0.9222\n",
      "Epoch 12/30\n",
      "42014/42014 [==============================] - 18s 433us/step - loss: 0.1769 - binary_accuracy: 0.9368 - val_loss: 0.2069 - val_binary_accuracy: 0.9220\n",
      "Epoch 13/30\n",
      "42014/42014 [==============================] - 18s 433us/step - loss: 0.1745 - binary_accuracy: 0.9373 - val_loss: 0.2071 - val_binary_accuracy: 0.9240\n",
      "Epoch 14/30\n",
      "42014/42014 [==============================] - 18s 431us/step - loss: 0.1693 - binary_accuracy: 0.9393 - val_loss: 0.2049 - val_binary_accuracy: 0.9251\n",
      "Epoch 15/30\n",
      "42014/42014 [==============================] - 18s 431us/step - loss: 0.1660 - binary_accuracy: 0.9408 - val_loss: 0.2089 - val_binary_accuracy: 0.9254\n",
      "Epoch 16/30\n",
      "42014/42014 [==============================] - 18s 426us/step - loss: 0.1625 - binary_accuracy: 0.9419 - val_loss: 0.2056 - val_binary_accuracy: 0.9248\n",
      "Epoch 17/30\n",
      "42014/42014 [==============================] - 19s 454us/step - loss: 0.1613 - binary_accuracy: 0.9419 - val_loss: 0.2069 - val_binary_accuracy: 0.9243\n",
      "Epoch 18/30\n",
      "42014/42014 [==============================] - 18s 438us/step - loss: 0.1583 - binary_accuracy: 0.9434 - val_loss: 0.2077 - val_binary_accuracy: 0.9260\n",
      "Epoch 19/30\n",
      "42014/42014 [==============================] - 21s 502us/step - loss: 0.1570 - binary_accuracy: 0.9440 - val_loss: 0.2061 - val_binary_accuracy: 0.9248\n",
      "Epoch 20/30\n",
      "42014/42014 [==============================] - 20s 473us/step - loss: 0.1539 - binary_accuracy: 0.9447 - val_loss: 0.2090 - val_binary_accuracy: 0.9255\n",
      "Epoch 21/30\n",
      "42014/42014 [==============================] - 19s 452us/step - loss: 0.1499 - binary_accuracy: 0.9466 - val_loss: 0.2096 - val_binary_accuracy: 0.9260\n",
      "Epoch 22/30\n",
      "42014/42014 [==============================] - 19s 457us/step - loss: 0.1496 - binary_accuracy: 0.9472 - val_loss: 0.2076 - val_binary_accuracy: 0.9241\n",
      "Epoch 23/30\n",
      "42014/42014 [==============================] - 18s 429us/step - loss: 0.1459 - binary_accuracy: 0.9483 - val_loss: 0.2088 - val_binary_accuracy: 0.9242\n",
      "Epoch 24/30\n",
      "42014/42014 [==============================] - 18s 433us/step - loss: 0.1443 - binary_accuracy: 0.9486 - val_loss: 0.2091 - val_binary_accuracy: 0.9247\n",
      "Epoch 25/30\n",
      "42014/42014 [==============================] - 18s 429us/step - loss: 0.1422 - binary_accuracy: 0.9497 - val_loss: 0.2114 - val_binary_accuracy: 0.9251\n",
      "Epoch 26/30\n",
      "42014/42014 [==============================] - 15s 364us/step - loss: 0.1399 - binary_accuracy: 0.9503 - val_loss: 0.2142 - val_binary_accuracy: 0.9255\n",
      "Epoch 27/30\n",
      "42014/42014 [==============================] - 14s 338us/step - loss: 0.1400 - binary_accuracy: 0.9509 - val_loss: 0.2143 - val_binary_accuracy: 0.9274\n",
      "Epoch 28/30\n",
      "42014/42014 [==============================] - 15s 348us/step - loss: 0.1357 - binary_accuracy: 0.9523 - val_loss: 0.2143 - val_binary_accuracy: 0.9257\n",
      "Epoch 29/30\n",
      "42014/42014 [==============================] - 16s 376us/step - loss: 0.1351 - binary_accuracy: 0.9526 - val_loss: 0.2162 - val_binary_accuracy: 0.9255\n",
      "Epoch 30/30\n",
      "42014/42014 [==============================] - 17s 395us/step - loss: 0.1392 - binary_accuracy: 0.9494 - val_loss: 0.2167 - val_binary_accuracy: 0.9267\n",
      "Loading trained model\n",
      "10504/10504 [==============================] - 1s 123us/step\n",
      "Test loss: 0.216706213368\n",
      "Test accuracy: 0.926694592536\n"
     ]
    }
   ],
   "source": [
    "# Fit the model on the batches generated by datagen.flow().\n",
    "model_dnn.fit(x_train, y_train,\n",
    "                    epochs=epochs,\n",
    "                    validation_data=(x_test, y_test),\n",
    ")\n",
    "#                     callbacks=[checkpoint, earlystop])\n",
    "# loading our save model\n",
    "print(\"Loading trained model\")\n",
    "# model = load_model(model_path)\n",
    "\n",
    "# Score trained model.\n",
    "scores = model_dnn.evaluate(x_test, y_test, verbose=1)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.90414734012296361"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model_dnn.predict(x_test)\n",
    "fpr, tpr, thresholds = sklearn.metrics.roc_curve(y_test, y_pred)\n",
    "sklearn.metrics.auc(fpr,tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#因為submission原本抓進來是按照FileID排序，但在前面merge的時候亂掉，這裡要先用sort把他換回來\n",
    "testing_data=testing_data.sort_values('FileID')\n",
    "#去除FileID跟label的欄位\n",
    "testing_set=testing_data.drop(['FileID','label'],axis=1)\n",
    "#轉成array（因為training好像是吃array）\n",
    "testing_set=testing_set.as_matrix()\n",
    "#predict_proba出來才會是百分比\n",
    "predict = model_dnn.predict_proba(testing_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FileID</th>\n",
       "      <th>Probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00008c73ee43c15b16c26b26398c1577</td>\n",
       "      <td>0.000227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0002ded3a0b54f2ffdab0ca77a5ce2b6</td>\n",
       "      <td>0.959523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00050a9df8e13f6ab5a3d3b3e2fc6a86</td>\n",
       "      <td>0.000079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000b1aa62b95e448784b8b341de46c64</td>\n",
       "      <td>0.007668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000d9f96b5eddf04a3b7a37cb95d0a00</td>\n",
       "      <td>0.137049</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             FileID  Probability\n",
       "0  00008c73ee43c15b16c26b26398c1577     0.000227\n",
       "1  0002ded3a0b54f2ffdab0ca77a5ce2b6     0.959523\n",
       "2  00050a9df8e13f6ab5a3d3b3e2fc6a86     0.000079\n",
       "3  000b1aa62b95e448784b8b341de46c64     0.007668\n",
       "4  000d9f96b5eddf04a3b7a37cb95d0a00     0.137049"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#predict\n",
    "submission['Probability']=predict\n",
    "submission=submission.drop(['label'],axis=1)\n",
    "submission.columns=['FileID','Probability']\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#匯出\n",
    "submission.to_csv('submission.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
